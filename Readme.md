\# ğŸ“š Multi-PDF RAG Chatbot using LLaMA 3 (Local)



This is a simple GenAI project that lets you \*\*chat with multiple PDFs\*\* using a \*\*locally hosted LLaMA 3 model\*\* via Ollama. It performs \*\*PDF text extraction, embedding with FAISS\*\*, and uses \*\*LangChain RetrievalQA\*\* for intelligent question-answering â€” all offline!



!\[Output Screenshot](output.png)



---



\## ğŸ› ï¸ Tech Stack



\- Python 3.10  

\- \[LangChain](https://github.com/langchain-ai/langchain)  

\- \[Ollama](https://ollama.com/) with `llama3` model  

\- FAISS (for vector search)  

\- PyMuPDF (`fitz`) for PDF text extraction  

\- `.env` for model config



---



\## ğŸš€ How to Run



1\. âœ… Make sure you have `ollama` installed and running locally:



&nbsp;  ollama run llama3



âœ… Install dependencies:



pip install -r requirements.txt



âœ… Place your PDFs in the data/ folder.



âœ… Run the app:

python app.py





\##ğŸ’¬ Ask questions about the PDFs!



\##ğŸ”® Future Enhancements:


ğŸŒ Web Interface using Flask / Streamlit



âš¡ Fast embeddings using InstructorXL, BGE, or GGUF models



ğŸ” Support live PDF uploads



ğŸ¤ Integrate with Chat history and memory



ğŸ“¦ Dockerize and deploy on GCP / AWS for scalable use



ğŸ§  Add summarization, keyword extraction, and PDF insights as tools

##ğŸ‘¨â€ğŸ’» Developed by

Guru Sai Sashank

Built as part of a GenAI portfolio.


# ğŸ§  Multi-PDF RAG Chatbot using LLaMA 3

This project is a local **RAG (Retrieval-Augmented Generation)** chatbot that can read and answer questions from multiple PDF files using **LLaMA 3** running on **Ollama**, without needing OpenAI API keys.

---

## âœ… Features

- ğŸ“„ Accepts multiple PDFs as knowledge base
- ğŸ§© Splits text into chunks for better context handling
- ğŸ“¦ Uses FAISS for vector storage and semantic retrieval
- ğŸ¤– Uses local **LLaMA 3** model for inference via **Ollama**
- ğŸ” Chat loop interface to interact with the data

---

## ğŸ› ï¸ Tech Stack

- Python
- LangChain
- Ollama (LLaMA 3)
- FAISS
- PyMuPDF
- dotenv

---

## ğŸ§ª How to Run

1. **Install Ollama & pull model**
   ```
   ollama run llama3
   ```

2. **Install dependencies**
   ```
   pip install -r requirements.txt
   ```

3. **Add PDF files to `data/` folder**
   ```
   ğŸ“ data/
   â”œâ”€â”€ resume.pdf
   â””â”€â”€ data.pdf
   ```

4. **Run the chatbot**
   ```
   python app.py
   ```

---

## ğŸ–¼ï¸ Sample Output

![Bot Output](output.png)

---

## ğŸš€ Future Enhancements

- Build a **web-based interface** using Streamlit or Flask
- Add a **file upload** feature for dynamic PDF processing
- Integrate **embedding caching** to avoid recomputation
- Optionally serve as a **local personal assistant bot**

---

## ğŸ§  Credits

- Built using [LangChain](https://github.com/langchain-ai/langchain)
- Embeddings and LLMs via [Ollama](https://ollama.com)

